{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>This is our simple demo file</h1>\n",
    "<h4>Running all cells will compute our approach for CIFAR-10 or MNIST (which will be downloaded)</h4>\n",
    "This code (except with modifications to use torch lightning) has been used to obtain all of our benchmarks as specified in the paper. We keep it in one simple Jupyter Notebook to make sure it stays accessible<br/>\n",
    "(and because \"we can't code properly\", according to a dear co-author).<br/>\n",
    "<u>At some point, we will turn this into a proper git repository</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> follow https://github.com/DaiDaiLoh/QG-VAE for the prettier version!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>!!! First few epochs use very few pseudo frequencies and can be hence <b>dreadful</b> to look at!!!</h1>\n",
    "We can only split Voronoi cells aka codewords that are in use, hence it takes a bit to use the full codebook. Improving that would do a lot - but we beat benchmarks anyway, so we left it like this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "torch.set_printoptions(precision=8, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    ### SETTINGS FOR VQVAE ###\n",
    "    VQVAE_D = 64 #dimensionality in which we cluster\n",
    "    VQVAE_K = 512 #number of different codewords (i.e. number of voronoi cells/clusters)\n",
    "    VQVAE_C = 8*8 #number of codewords to describe one encoded item\n",
    "    DS_SPEED_FACTOR = 0 #number of times we downscale our feature map; called \"f\" in the paper\n",
    "    USE_DROPOUT = False #if we want to order our latent code from important to unimportant \n",
    "\n",
    "    ### SETTINGS FOR NETWORKS ###\n",
    "    CHANNELS_MAXIMUM = 128 #maximum number of channels in our U-Nets\n",
    "    CHANNELS_MINIMUM = 16 #minimum number of channels we output in U-Nets\n",
    "    NUM_WORKERS = 0 #for linux, put to something higher, but windows does not like this for some reason\n",
    "    RES_BLOCKS = 1\n",
    "    DIM_L_EMBED = 16\n",
    "    DO_QUANTISE = True\n",
    "    NUM_HEADS = 8\n",
    "\n",
    "    NUM_TRAIN_BATCHES_UNTIL_RESET = 100 #reset unused entries every 100 epochs\n",
    "\n",
    "    ### RUN SETTINGS ###\n",
    "    DATASET = [\"CIFAR\", \"MNIST\"][0]\n",
    "\n",
    "    CLUSTERRUN = False\n",
    "\n",
    "    RUN_NAME = \"\" #is set later automatically\n",
    "\n",
    "    USE_CHECKPOINTS = False\n",
    "\n",
    "SETTINGS = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS.RUN_NAME = SETTINGS.DATASET + (\"_dropout\" if SETTINGS.USE_DROPOUT else \"\")\n",
    "print(\"RUN NAME: \",SETTINGS.RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(\"outputs/\")\n",
    "mkdir(\"outputs/\"+SETTINGS.RUN_NAME+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not SETTINGS.USE_DROPOUT or SETTINGS.USE_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTINGS.DATASET == \"CIFAR\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=SETTINGS.NUM_WORKERS)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=SETTINGS.NUM_WORKERS)\n",
    "elif SETTINGS.DATASET == \"MNIST\":\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=SETTINGS.NUM_WORKERS)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=SETTINGS.NUM_WORKERS)\n",
    "else:\n",
    "    print(\"INVALID DATASET CHOSEN!\")\n",
    "    assert(False)\n",
    "\n",
    "def imshow(img):\n",
    "    #check if is in interactive session:\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        npimg = img.clamp(0.0, 1.0).numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "def save_image(img, name):\n",
    "    torchvision.utils.save_image(img, name)\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    print(\"IMAGE VALUE RANGES: \",images.min(), \" to \",images.max())\n",
    "    print(\"IMAGE SHAPE: \",images.shape)\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    save_image(torchvision.utils.make_grid(images), \"outputs/\"+SETTINGS.RUN_NAME+\"/test.png\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x, DIM_L_EMBED):\n",
    "    rets = []\n",
    "    for i in range(DIM_L_EMBED):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            rets.append(fn((2. ** i) * x))\n",
    "    return torch.cat(rets, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(SETTINGS.VQVAE_C % SETTINGS.NUM_HEADS == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS.TRAIN_BATCHES = len(trainloader)\n",
    "SETTINGS.TEST_BATCHES = len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in trainloader:\n",
    "    SETTINGS.INPUT_C = images.size()[1]\n",
    "    SETTINGS.INPUT_W = images.size()[2]\n",
    "    break\n",
    "SETTINGS.INPUT_H = SETTINGS.INPUT_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallResidualBlock(nn.Module):\n",
    "    #basically, this is the Imagen residual Block\n",
    "    def __init__(self, c_in, c_int, c_out, res_dims, relu_slope=0.01):\n",
    "        super(SmallResidualBlock, self).__init__()\n",
    "\n",
    "        n1_no_grp = 16\n",
    "        n2_no_grp = 16\n",
    "        while (c_in) % n1_no_grp != 0:\n",
    "            n1_no_grp -= 1\n",
    "        while (c_int) % n2_no_grp != 0:\n",
    "            n2_no_grp -= 1\n",
    "        assert(n1_no_grp > 0 and n2_no_grp > 0)\n",
    "        self.norm1 = nn.GroupNorm(num_groups=n1_no_grp, num_channels=c_in, eps=1e-05, affine=True)\n",
    "        self.norm2 = nn.GroupNorm(num_groups=n2_no_grp, num_channels=c_int, eps=1e-05, affine=True)\n",
    "\n",
    "        self.conv_res = nn.Conv2d(kernel_size=1, in_channels=c_in, out_channels=c_out, stride=1, padding=0, bias=False)\n",
    "        self.conv_res_conditional = nn.Conv2d(kernel_size=1, in_channels=res_dims, out_channels=c_in, stride=1, padding=0, bias=False)\n",
    "        self.conv1 = nn.Conv2d(kernel_size=3, in_channels=c_in, out_channels=c_int, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(kernel_size=3, in_channels=c_int, out_channels=c_out, stride=1, padding=1)\n",
    "        \n",
    "        self.relu = nn.LeakyReLU(negative_slope=relu_slope)\n",
    "        self.activation = nn.SiLU(inplace=False)\n",
    "\n",
    "    def forward(self, x, x_conditional_res):\n",
    "        x_res = self.conv_res(x)\n",
    "\n",
    "        x = x + self.conv_res_conditional(x_conditional_res)\n",
    "        #according to Google's Imagen:\n",
    "        x = self.activation(self.norm1(x))\n",
    "        x = self.activation(self.norm2(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = x + x_res\n",
    "        return x\n",
    "    \n",
    "class ResidualBlocks(nn.Module):\n",
    "    def __init__(self, c_in, c_int, c_out, res_dims, relu_slope=0.01, RES_BLOCKS=3):\n",
    "        super(ResidualBlocks, self).__init__()\n",
    "\n",
    "        #list of small residual blocks #\n",
    "        self.blocks = nn.ModuleList()\n",
    "        # use exactly THREE res blocks!\n",
    "        for i in range(0, RES_BLOCKS):\n",
    "            cur_c_in, cur_c_out = c_int, c_int\n",
    "            if i == 0:\n",
    "                cur_c_in = c_in\n",
    "            if i == RES_BLOCKS - 1:\n",
    "                cur_c_out = c_out\n",
    "            self.blocks.append(SmallResidualBlock(cur_c_in, c_int, cur_c_out, res_dims=res_dims, relu_slope=relu_slope))\n",
    "\n",
    "    def forward(self, x, x_conditional_res):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, x_conditional_res)\n",
    "        return x\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in, c_int, width_height, NO_CHANNELS_MAX, NO_CHANNELS_MIN, res_dims, relu_slope=0.01, current_layer=0, skip_up_layers=0, skip_down_layers=0, target_out=None):\n",
    "        super(UNetBlock, self).__init__()\n",
    "\n",
    "        self.in_conv = nn.Conv2d(kernel_size=3, in_channels=c_in, out_channels=c_int, stride=1, padding=1)\n",
    "        c_next = min(c_int * 2, NO_CHANNELS_MAX)\n",
    "        self.in_res = ResidualBlocks(c_int, c_int, c_out=c_next, res_dims=res_dims, relu_slope=relu_slope, RES_BLOCKS=SETTINGS.RES_BLOCKS)\n",
    "\n",
    "        self.width_height = width_height\n",
    "\n",
    "        if self.width_height > 8 or (skip_up_layers > current_layer or current_layer < skip_down_layers): #don't go below 8x8, that makes no sense (except if we have to)\n",
    "            self.unet = UNetBlock(SETTINGS, c_in=c_next, c_int=c_next, width_height=int(width_height/2), NO_CHANNELS_MAX=NO_CHANNELS_MAX, NO_CHANNELS_MIN=NO_CHANNELS_MIN, res_dims=res_dims, relu_slope=relu_slope, current_layer=current_layer+1, skip_up_layers=skip_up_layers, skip_down_layers=skip_down_layers)\n",
    "            self.downsample = nn.Upsample(scale_factor=0.5, mode='bilinear')\n",
    "            self.downsample_cond = nn.Upsample(scale_factor=0.5, mode='nearest')\n",
    "            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "            next_input = self.unet.out_conv.out_channels\n",
    "        else:\n",
    "            self.unet = None\n",
    "            next_input = c_next\n",
    "\n",
    "        dim_out = c_int\n",
    "        if skip_up_layers > current_layer:\n",
    "            dim_out = self.unet.out_conv.out_channels\n",
    "        \n",
    "        if current_layer == 0 and target_out != None:\n",
    "            dim_out = target_out\n",
    "        \n",
    "        self.out_res = ResidualBlocks(next_input, max(c_int, dim_out), c_out=dim_out, res_dims=res_dims, relu_slope=relu_slope, RES_BLOCKS=SETTINGS.RES_BLOCKS)\n",
    "        self.out_conv = nn.Conv2d(kernel_size=3, in_channels=dim_out, out_channels=dim_out, stride=1, padding=1)\n",
    "        self.full_residual = nn.Conv2d(kernel_size=1, in_channels=c_in, out_channels=dim_out, stride=1, padding=0)\n",
    "        \n",
    "        if current_layer < skip_down_layers:\n",
    "            #make sure we do SHRINK the number of parameters!\n",
    "            if self.unet == None:\n",
    "                prev_ch = c_next\n",
    "            else:\n",
    "                prev_ch = int(self.unet.out_conv.out_channels)\n",
    "            layers_to_go = min(NO_CHANNELS_MIN * (2 ** current_layer), prev_ch)\n",
    "            if current_layer == 0 and target_out != None:\n",
    "                layers_to_go = target_out\n",
    "            \n",
    "            self.out_res = ResidualBlocks(next_input, next_input, c_out=layers_to_go, res_dims=res_dims, relu_slope=relu_slope, RES_BLOCKS=SETTINGS.RES_BLOCKS)\n",
    "            self.out_conv = nn.Conv2d(kernel_size=3, in_channels=layers_to_go, out_channels=layers_to_go, stride=1, padding=1)\n",
    "            self.full_residual = nn.Conv2d(kernel_size=1, in_channels=c_in, out_channels=layers_to_go, stride=1, padding=0)\n",
    "        \n",
    "        self.activation = nn.SiLU(inplace=False)\n",
    "\n",
    "        self.current_layer = current_layer\n",
    "        self.skip_up_layers = skip_up_layers\n",
    "        self.skip_down_layers = skip_down_layers\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        #input block\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        #apply (multiple?) residual blocks\n",
    "        x = self.in_res(x, residual)\n",
    "        if x.size()[2] > 1 and self.unet != None:\n",
    "            #downscale\n",
    "            if self.skip_down_layers <= self.current_layer:\n",
    "                x = self.downsample(x)\n",
    "            ds_res = residual\n",
    "            while x.size()[2] < ds_res.size()[2]:\n",
    "                ds_res = self.downsample_cond(ds_res)\n",
    "            #recursive\n",
    "            x = self.unet(x, ds_res)\n",
    "            #upscale\n",
    "            if self.skip_up_layers <= self.current_layer:\n",
    "                x = self.upsample(x)\n",
    "                \n",
    "        #output residual\n",
    "        while x.size()[2] > residual.size()[2]:\n",
    "            residual = self.upsample(residual)\n",
    "        while x.size()[2] < residual.size()[2]:\n",
    "            residual = self.downsample(residual)\n",
    "\n",
    "        x = self.out_res(x, residual)\n",
    "        #convolution\n",
    "        x = self.out_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in, c_int, c_out, NO_CHANNELS_MAX, NO_CHANNELS_MIN, width_height, skip_up_layers=0, skip_down_layers=0):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        assert(skip_up_layers == 0 or skip_down_layers == 0)\n",
    "\n",
    "        self.SETTINGS = SETTINGS\n",
    "        res_dims = 2 * 2 * self.SETTINGS.DIM_L_EMBED + c_in\n",
    "        self.position = nn.ParameterList()\n",
    "        wh = width_height\n",
    "        self.wh = width_height\n",
    "        while wh >= 1:\n",
    "            self.position.append(nn.Parameter(self.grid_positional_encoding(wh)[None], requires_grad=False))\n",
    "            wh = int(wh / 2)\n",
    "        \n",
    "        self.unet_blocks = UNetBlock(SETTINGS, c_in, c_int, width_height=width_height, NO_CHANNELS_MAX=NO_CHANNELS_MAX, NO_CHANNELS_MIN=NO_CHANNELS_MIN, res_dims=res_dims, skip_up_layers=skip_up_layers, skip_down_layers=skip_down_layers, target_out=c_out)\n",
    "\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode='bilinear')\n",
    "        self.upsample = nn.Upsample(scale_factor=2.0, mode='bilinear')\n",
    "        \n",
    "        self.mega_residual = torch.nn.Conv2d(kernel_size=3, in_channels=c_in, out_channels=self.unet_blocks.out_conv.out_channels, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.out_projection = nn.Conv2d(kernel_size=3, in_channels=self.unet_blocks.out_conv.out_channels, out_channels=c_out, stride=1, padding=1, bias=False)\n",
    "        \n",
    "    def grid_positional_encoding(self, width_height):\n",
    "        grid = torch.ones(width_height, width_height, 2)\n",
    "        for x in range(0, width_height):\n",
    "            grid[x,:,0] = x / width_height\n",
    "        for y in range(0, width_height):\n",
    "            grid[:,y,1] = y / width_height\n",
    "        #first dimension is what we should encode\n",
    "        rets = []\n",
    "        for i in range(self.SETTINGS.DIM_L_EMBED):\n",
    "            for fn in [torch.sin, torch.cos]:\n",
    "                rets.append(fn((2. ** i) * grid))\n",
    "        return torch.cat(rets, -1).transpose(2,1).transpose(1,0)\n",
    "\n",
    "    #decode in the sense of: turn into embeddings\n",
    "    def decode_indices(self, indices, codebook):\n",
    "        w, h = indices.size()[1], indices.size()[2]\n",
    "        output = codebook.transpose(0,1)[indices.view(-1)]\n",
    "        output = output.transpose(0,1)\n",
    "        output = output.view(output.size()[0], indices.size()[0], w, h).transpose(0,1)\n",
    "        return output\n",
    "    \n",
    "    def get_positions(self, width_height):\n",
    "        positions = []\n",
    "        cur_wh = width_height\n",
    "        while cur_wh >= 1:\n",
    "            ### use only relative position\n",
    "            diff = self.wh - cur_wh\n",
    "            if diff == 0:\n",
    "                index = 0\n",
    "            else:\n",
    "                index = int(math.log2(self.wh)) - int(math.log2(cur_wh)) #0 at first\n",
    "            position_absolute = self.position[index].clone()\n",
    "            cur_wh = int(cur_wh / 2)\n",
    "            positions.append(position_absolute)\n",
    "        return positions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = torch.cat((x, self.position[0].repeat(x.size()[0], 1, 1, 1)), 1)\n",
    "        mega_res = self.mega_residual(x)\n",
    "        #downscale:\n",
    "        out = self.unet_blocks(x, residual)\n",
    "\n",
    "        while out.size()[2] > mega_res.size()[2]:\n",
    "            mega_res = self.upsample(mega_res)\n",
    "        while out.size()[2] < mega_res.size()[2]:\n",
    "            mega_res = self.downsample(mega_res)\n",
    "        result = self.out_projection(out + mega_res)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode='bilinear')\n",
    "\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=c_in, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(self.downsample)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(self.downsample)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.VQVAE_C, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, c_out):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2.0, mode='bilinear')\n",
    "\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.VQVAE_C, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(self.upsample)\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(self.upsample)\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=c_out, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu) #only because we have the output values in [0, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in, c_int, width_height, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        #c_in, c_int, c_out, res_dims=0, relu_slope=0.01\n",
    "        self.block_in = SmallResidualBlock(c_in, c_int, c_int, res_dims=c_in, relu_slope=0.2)\n",
    "        self.unet_blocks = UNetBlock(SETTINGS, c_int, c_int, c_int, width_height=width_height, NO_CHANNELS_MAX=c_int, NO_CHANNELS_MIN=16, res_dims=input_channels, relu_slope=0.2)\n",
    "        self.block_out = SmallResidualBlock(c_int, c_int, 1, res_dims=c_in, relu_slope=0.2)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        #in\n",
    "        x = self.block_in(x, x_input)\n",
    "        #unet\n",
    "        x = self.unet_blocks(x, x_input)\n",
    "        #residual after input layer\n",
    "        x = self.block_out(x, x_input)\n",
    "        #patchwise discriminator\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fullsize(x):\n",
    "    if len(x.size()) == 1:\n",
    "        return \"ERROR: get_fullsize() called with 1D tensor\"\n",
    "    elif len(x.size()) == 2:\n",
    "        return x.size()[1]\n",
    "    elif len(x.size()) == 3:\n",
    "        return x.size()[1]*x.size()[2]\n",
    "    elif len(x.size()) == 4:\n",
    "        return x.size()[1]*x.size()[2]*x.size()[3]\n",
    "    elif len(x.size()) == 5:\n",
    "        return x.size()[1]*x.size()[2]*x.size()[3]*x.size()[4]\n",
    "    else:\n",
    "        print(\"ERROR: get_fullsize() called with tensor of size \", x.size())\n",
    "\n",
    "class BaseAE(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in):\n",
    "        super(BaseAE, self).__init__()\n",
    "\n",
    "        self.SETTINGS = SETTINGS\n",
    "        self.relu = torch.nn.LeakyReLU() #we use leaky relu as activation function, as it also has a gradient for an input < 0\n",
    "        self.reduce = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.upscale = torch.nn.Upsample(scale_factor=2.0, mode='bilinear')\n",
    "\n",
    "        self.residual_encoder = torch.nn.Conv2d(kernel_size=1, in_channels=SETTINGS.INPUT_C, out_channels=SETTINGS.VQVAE_C, stride=1, padding=0)\n",
    "        self.residual_decoder = torch.nn.Conv2d(kernel_size=1, in_channels=SETTINGS.VQVAE_C, out_channels=SETTINGS.INPUT_C, stride=1, padding=0)\n",
    "\n",
    "        self.encoder = UNet(SETTINGS, c_in=c_in, c_int=SETTINGS.CHANNELS_MINIMUM, c_out=SETTINGS.VQVAE_C, NO_CHANNELS_MAX=SETTINGS.CHANNELS_MAXIMUM, NO_CHANNELS_MIN=SETTINGS.CHANNELS_MINIMUM, width_height=SETTINGS.INPUT_W, skip_up_layers=SETTINGS.DS_SPEED_FACTOR)\n",
    "        \n",
    "        #create a list of projections:\n",
    "        self.down_proj = nn.ModuleList()\n",
    "        self.up_proj   = nn.ModuleList()\n",
    "        for i in range(0, SETTINGS.NUM_HEADS):\n",
    "            self.down_proj.append(torch.nn.Conv1d(in_channels=int(SETTINGS.INPUT_W*SETTINGS.INPUT_H/(2 ** SETTINGS.DS_SPEED_FACTOR)/(2 ** SETTINGS.DS_SPEED_FACTOR)), out_channels=SETTINGS.VQVAE_D, kernel_size=1, stride=1, padding=0))\n",
    "            self.up_proj.append(torch.nn.Conv1d(in_channels=SETTINGS.VQVAE_D, out_channels=int(SETTINGS.INPUT_W*SETTINGS.INPUT_H/(2 ** SETTINGS.DS_SPEED_FACTOR)/(2 ** SETTINGS.DS_SPEED_FACTOR)), kernel_size=1, stride=1, padding=0))\n",
    "        self.decoder = UNet(SETTINGS, c_in=SETTINGS.VQVAE_C, c_int=SETTINGS.CHANNELS_MAXIMUM, c_out=c_in, NO_CHANNELS_MAX=SETTINGS.CHANNELS_MAXIMUM, NO_CHANNELS_MIN=SETTINGS.CHANNELS_MINIMUM, width_height=int(SETTINGS.INPUT_W/(2 ** SETTINGS.DS_SPEED_FACTOR)), skip_down_layers=SETTINGS.DS_SPEED_FACTOR)\n",
    "\n",
    "        #use individual codebooks per pseudo frequency:\n",
    "        self.codebook = torch.nn.Parameter((torch.rand(SETTINGS.VQVAE_C, SETTINGS.VQVAE_K, SETTINGS.VQVAE_D) * 2.0 - 1.0) * 0.5, requires_grad=True)\n",
    "        self.offset = torch.nn.Parameter((torch.rand(SETTINGS.VQVAE_D, SETTINGS.VQVAE_C) * 2.0 - 1.0) * 0.5, requires_grad=True)\n",
    "\n",
    "        self.pos = positional_encoding(torch.arange(0, SETTINGS.VQVAE_C)[:,None] / (SETTINGS.VQVAE_C+1), 10)[None,:,:].transpose(1,2)\n",
    "\n",
    "    def quantise_individually(self, x):\n",
    "        x_before_rounding = x.clone()\n",
    "        #size  x in: [b x D x C x 1]\n",
    "        #size cb in: [C x K x D]\n",
    "        x_before_rounding = x.clone()\n",
    "\n",
    "        ### torch.rand(C, D, K)\n",
    "        B = x.size()[0]\n",
    "        x = x.transpose(1,2)[:,:,:,0]\n",
    "        codebook = self.codebook.transpose(1,2)\n",
    "\n",
    "        dists = (x[..., None] - codebook[None]).square().sum(-2)#\n",
    "        idx = dists.argmin(-1)\n",
    "        \n",
    "        codebook_expanded = codebook[None].expand(B, -1, -1, -1)\n",
    "        idx_expanded = idx[..., None, None].expand(-1, -1, self.SETTINGS.VQVAE_D, -1)\n",
    "        \n",
    "        x_rounded = torch.gather(codebook_expanded, -1, idx_expanded).transpose(1,2)\n",
    "\n",
    "        loss_commitment = (x_before_rounding - x_rounded.detach()).square().mean() #only have gradient for encoder \n",
    "        loss_codebook   = (x_before_rounding.detach() - x_rounded).square().mean() #only have gradient for codebook\n",
    "\n",
    "        x = x_before_rounding - (x_before_rounding - x_rounded).detach()\n",
    "        \n",
    "        return x, loss_codebook, loss_commitment, idx.view(x.size()[0], x.size()[2], x.size()[3])\n",
    "    \n",
    "    def quantise(self, x):\n",
    "        #this is where the magic happens:\n",
    "        #    1. take input of size [b x W x H x VQVAE_D] and reshape to [b x VQVAE_D x W x H ];\n",
    "        #       this means we have ALL the codewords in the last dimension now, so we can just merge together all other dimensions to easily compare them to the codewords\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        size_in = x.size()\n",
    "        x = x.view(-1, self.SETTINGS.VQVAE_D)\n",
    "        #   2. compare [b*W*H x VQVAE_D] to [CODEWORDS x VQVAE_D]: by re-shaping to have [b*W*H x 1 x VQVAE_D] and [1 x CODEWORDS x VQVAE_D], the substraction will\n",
    "        #      give us a [b*W*H x CODEWORDS X VQVAE_D] tensor of all distance pairs between input elements (b*W*H) and the CODEWORDS-many codewords; just square, \n",
    "        #      then sum up the last dimension to get sum of the distance values for each dimension (=euclidean squared distance)\n",
    "        #      and then take the argmin to get the index of the closest codeword for each input element - these are our indices we want!\n",
    "        if self.SETTINGS.DATASET != \"IMAGENET\":\n",
    "            with torch.no_grad():\n",
    "                indices = (x[:,None] - self.codebook.view(1, self.SETTINGS.VQVAE_K, self.SETTINGS.VQVAE_D)).square().sum(dim=2).argmin(dim=1)\n",
    "        else:\n",
    "            #quantise in chunks\n",
    "            chunksize = int(x.size()[0]/8)\n",
    "            with torch.no_grad():\n",
    "                index_begin = 0\n",
    "                index_end   = chunksize\n",
    "                acc_indices = []\n",
    "                while index_begin < x.size()[0]:\n",
    "                    indices = (x[index_begin:index_end][:,None] - self.codebook.view(1, self.SETTINGS.VQVAE_K, self.SETTINGS.VQVAE_D)).square().sum(dim=2).argmin(dim=1)\n",
    "                    acc_indices.append(indices)\n",
    "                    index_begin += chunksize\n",
    "                    index_end += chunksize\n",
    "                indices = torch.cat(acc_indices)\n",
    "            \n",
    "        #   3. we take those indices we just looked up: these are the indices of the closest codeword for each input element, so we can now look up the actual \n",
    "        #      codeword to find the closest codeword to each input element aka the \"centroids\" we round to\n",
    "        x_rounded = self.codebook[indices]\n",
    "        \n",
    "        #   4. we calculate the loss for the commitment loss (i.e. does the encoder produce stuff from the codebook?) and the codebook loss (how close is the codebook to the encoder outputs?)\n",
    "        #      the value is exactly the same for both terms, but this way we can a) train encoder and codebook with different magnitudes and b) the encoder outputs and codebook outputs are\n",
    "        #      not just \"shrinked\" to one super tiny value: the loss we be almost zero if codebook and encoder outputs would be just downscaled by some tiny factor (latent space just shrinks together)\n",
    "        #      to avoid that, we use the detach() function, making sure that codebook and encoder outputs move individually and not just shrink together to something tiny\n",
    "        loss_commitment = (x - x_rounded.detach()).square().mean() #only have gradient for encoder \n",
    "        loss_codebook   = (x.detach() - x_rounded).square().mean() #only have gradient for codebook\n",
    "\n",
    "        #   5. the actual rounding we do: we take the difference between the rounded and the unrounded value and add it to the unrounded value\n",
    "        #      the paper calls this \"straight through estimator\", as we just pass the gradient through the rounding operation (which is not differentiable)\n",
    "        #      what essentially happens here is that we just take our x and substract some float without any gradient, similar to writing \"x = x - 0.1\":\n",
    "        #      \"(x - x_rounded).detach()\" becomes just some number that we substract from x; this converts x to the same values as x_rounded, but the gradient is not passed through this operation\n",
    "        #      i.e. we round, but keep the (then a bit inexact) gradient \n",
    "        x = x - (x - x_rounded).detach() #change x to x_rounded, but keep the gradient from x\n",
    "        #   6. we reshape back to the original shape and return the values; we also return both losses and the indices of the closest codewords\n",
    "        x = x.view(size_in)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x, loss_codebook, loss_commitment, indices.view(x.size()[0], x.size()[2], x.size()[3])\n",
    "    \n",
    "    def encode(self, x):\n",
    "        #1. encode\n",
    "        res = self.residual_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        while x.size()[2] < res.size()[2]:\n",
    "            res = self.reduce(res)\n",
    "        x = x + res #self.upscale\n",
    "        #2. re-shape and swap dimensions\n",
    "        x = x.view(x.size()[0], self.SETTINGS.VQVAE_C, -1)\n",
    "        x = x.transpose(1,2)\n",
    "        #3. project down\n",
    "        w = x.size()[2]\n",
    "        x_out = None\n",
    "        for i in range(0, SETTINGS.NUM_HEADS):\n",
    "            out = self.down_proj[i](x[:,:,int(w/SETTINGS.NUM_HEADS*(i)):int(w/SETTINGS.NUM_HEADS*(i+1))])\n",
    "            if x_out == None:\n",
    "                x_out = out\n",
    "            else:\n",
    "                x_out = torch.cat((x_out, out), 2)\n",
    "        x = x_out\n",
    "\n",
    "        #4. quantise\n",
    "        x = x.contiguous().view(x.size()[0], self.SETTINGS.VQVAE_D, -1, 1) #unfold, here just for quantisation\n",
    "        if not SETTINGS.DO_QUANTISE:\n",
    "            x_ = x.clone()\n",
    "        x, loss_codebook, loss_commitment, indices = self.quantise_individually(x)\n",
    "        \n",
    "        if not SETTINGS.DO_QUANTISE:\n",
    "            x = x_.clone()\n",
    "\n",
    "        #don't add individual offsets, but use the global one\n",
    "        x = x + self.offset[None, :, :, None]\n",
    "        return x, loss_codebook, loss_commitment, indices\n",
    "\n",
    "    def decode(self, x, w, h):\n",
    "        x = x.view(x.size()[0], self.SETTINGS.VQVAE_D, -1) #fold back after quantisation; REDUNDANT!!! only for using only decoder\n",
    "        #5. project up\n",
    "        x_out = None\n",
    "        blocksize = int(x.size()[2]/SETTINGS.NUM_HEADS)\n",
    "        for i in range(0, SETTINGS.NUM_HEADS):\n",
    "            out = self.up_proj[i](x[:,:,(blocksize*i):(blocksize * (i+1))])\n",
    "            if x_out == None:\n",
    "                x_out = out\n",
    "            else:\n",
    "                x_out = torch.cat((x_out, out), 2)\n",
    "        x = x_out\n",
    "        \n",
    "        #6. reshape back\n",
    "        x = x.transpose(1,2) #swap global information and local information back again (channels vs spatial dimensions)\n",
    "        x = x.contiguous().view(x.size()[0], self.SETTINGS.VQVAE_C, int(w/(2 ** self.SETTINGS.DS_SPEED_FACTOR)), int(h/(2 ** self.SETTINGS.DS_SPEED_FACTOR)))\n",
    "        #7. decode\n",
    "        res = self.residual_decoder(x)\n",
    "        x = self.decoder(x)\n",
    "        while x.size()[2] > res.size()[2]:\n",
    "            res = self.upscale(res)\n",
    "        x = x + res\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, dropout_mask=None):\n",
    "        #encode, then quantise, then decode; pass the codebook loss and commitment loss through so we can use them for training\n",
    "        w, h = x.size()[2], x.size()[3]\n",
    "        x, loss_codebook, loss_commitment, indices = self.encode(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], self.SETTINGS.VQVAE_D, -1) #fold back after quantisation\n",
    "        if dropout_mask != None:\n",
    "            x = x * dropout_mask[:,None,:]\n",
    "        \n",
    "        x = self.decode(x, w, h)\n",
    "\n",
    "        return x, loss_codebook, loss_commitment, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = BaseAE(SETTINGS, c_in=SETTINGS.INPUT_C)\n",
    "#sanity check if everything works\n",
    "for a, b in trainloader:\n",
    "    ae(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>we somehow messed up our performance when transferring back from lightning...<br/>\n",
    "will be fixed later, but at least it still runs I guess...</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPSILON = 0.000001\n",
    "\n",
    "ae = BaseAE(SETTINGS, c_in=SETTINGS.INPUT_C).to(DEVICE)\n",
    "optimiser = torch.optim.AdamW(ae.parameters(), lr=0.0002, weight_decay=0.01)\n",
    "\n",
    "index_count = torch.zeros(SETTINGS.VQVAE_C, SETTINGS.VQVAE_K).to(DEVICE)\n",
    "magnitudes = torch.zeros(SETTINGS.VQVAE_C, SETTINGS.VQVAE_K).to(DEVICE)\n",
    "\n",
    "#just for tracking stuff\n",
    "rec_loss = []\n",
    "last_output = None\n",
    "last_output = None\n",
    "\n",
    "steps = 0\n",
    "for epoch in range(0, 100):\n",
    "    losses = [0.0, 0.0, 0.0]\n",
    "    #train\n",
    "    this_epoch = 0\n",
    "    batchidx = 0\n",
    "    epoch_start_time = time.time()\n",
    "    print(\"*** Starting to train epoch \",epoch,\" ***\")\n",
    "    for data, _ in trainloader:\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        data = data.to(DEVICE)\n",
    "        out, loss_codebook, loss_commitment, indices = ae(data)\n",
    "\n",
    "        #######################\n",
    "        ### actual training ###\n",
    "        #######################\n",
    "        if True:\n",
    "            if SETTINGS.USE_DROPOUT:\n",
    "                dropout_mask_binary = torch.randint(0, 3, (data.size()[0],))\n",
    "                dropout_mask = torch.ones(data.size()[0], SETTINGS.VQVAE_C, device=data.device)\n",
    "                for i in range(dropout_mask.size()[0]):\n",
    "                    if dropout_mask_binary[i] == 0:\n",
    "                        dropout_mask[i,torch.randint(1, SETTINGS.VQVAE_C, (1,)).item():] = 0.0\n",
    "            else:\n",
    "                dropout_mask = None\n",
    "            \n",
    "            outputs, loss_codebook, loss_commitment, indices = ae(data, dropout_mask)\n",
    "            \n",
    "            indices = indices[:,:,0]\n",
    "\n",
    "            if True:\n",
    "                onehot = torch.nn.functional.one_hot(indices.detach(), num_classes=SETTINGS.VQVAE_K)\n",
    "                onehot = onehot.sum(dim=0)\n",
    "                #now shape [256 x 512]\n",
    "                index_count += onehot\n",
    "\n",
    "            #MSE\n",
    "            loss_reconstruction = (outputs - data).square()\n",
    "            loss_reconstruction = loss_reconstruction.mean()\n",
    "        \n",
    "            loss_non_rec = (loss_codebook * 0.25 + loss_commitment)\n",
    "            if not SETTINGS.DO_QUANTISE: #if we don't quantise, don't do anything\n",
    "                loss_non_rec = loss_non_rec * 0.0\n",
    "            loss = loss_reconstruction + loss_non_rec\n",
    "        \n",
    "        #######################\n",
    "        #### loss tracking ####\n",
    "        #######################\n",
    "        if True:\n",
    "            losses[0] += loss_reconstruction.item()\n",
    "            losses[1] += loss_codebook.item()\n",
    "            losses[2] += loss_commitment.item()\n",
    "            \n",
    "            rec_loss.append(loss_reconstruction.item())\n",
    "            rec_loss = rec_loss[-100:]\n",
    "            \n",
    "            if this_epoch == 100:\n",
    "                time_taken = time.time() - epoch_start_time\n",
    "                time_per_it = time_taken / this_epoch\n",
    "                print(\"\\t\\tProjected total time for this epoch: \",((SETTINGS.TRAIN_BATCHES+SETTINGS.TEST_BATCHES) * time_per_it),\" seconds\")\n",
    "        \n",
    "            if (last_output == None or (time.time() > last_output + 20.0)) and batchidx > 100:\n",
    "                print(\"\\t\\t\\t\"+str(batchidx/SETTINGS.TRAIN_BATCHES*100)+\"% done, current running mean: \"+str(sum(rec_loss)/len(rec_loss)))\n",
    "                print(\"\\t\\t\\t--> \"+str((time.time() - epoch_start_time) / (batchidx/SETTINGS.TRAIN_BATCHES) - (time.time() - epoch_start_time))+\" seconds left until epoch is concluded\")\n",
    "                last_output = time.time()\n",
    "\n",
    "        loss.backward()\n",
    "        magnitudes += ae.codebook.grad.abs().mean(dim=2).detach()\n",
    "        optimiser.step()\n",
    "\n",
    "        steps += 1\n",
    "        batchidx += 1\n",
    "\n",
    "        #######################\n",
    "        ### codebook resets ###\n",
    "        #######################\n",
    "        if steps % SETTINGS.NUM_TRAIN_BATCHES_UNTIL_RESET == 0:\n",
    "            print(\"\\t\\t---> RE-ALIGNING FREQUENCIES\")\n",
    "            \n",
    "            re_aligned_frequencies_total = []\n",
    "            for frequency in range(0, SETTINGS.VQVAE_C):\n",
    "                #1. while there exist codewords with magnitude = 0, re-initialise them\n",
    "                re_aligned_frequencies = 0\n",
    "                while True:\n",
    "                    #find a codeword to re-distribute\n",
    "                    unused_codeword = magnitudes[frequency].argmin()\n",
    "                    if magnitudes[frequency, unused_codeword] > 0:\n",
    "                        break\n",
    "\n",
    "                    #find a codeword to split up\n",
    "                    replacement_codeword = magnitudes[frequency].argmax()\n",
    "                    if magnitudes[frequency, replacement_codeword] <= EPSILON:\n",
    "                        break\n",
    "\n",
    "                    ### torch.zeros(self.SETTINGS.VQVAE_C, self.SETTINGS.VQVAE_K, self.SETTINGS.VQVAE_D)\n",
    "                    #re-distribute the codeword: find the direction with the highest magnitude and move the unused codeword in that direction\n",
    "                    direction = (torch.rand(SETTINGS.VQVAE_D) * 2.0 - 1.0) * 0.0001\n",
    "                    with torch.no_grad():\n",
    "                        ae.codebook[frequency, unused_codeword] = ae.codebook[frequency, replacement_codeword].to(ae.codebook[frequency, unused_codeword].get_device()) + direction.to(ae.codebook[frequency, unused_codeword].get_device())\n",
    "\n",
    "                    #set the replacement codeword/unused codeword to EPSILON to make sure that we don't use it again\n",
    "                    magnitudes[frequency, replacement_codeword] = EPSILON\n",
    "                    magnitudes[frequency, unused_codeword]      = EPSILON\n",
    "                    re_aligned_frequencies += 1\n",
    "                re_aligned_frequencies_total.append(re_aligned_frequencies)\n",
    "                \n",
    "                #set back to zero for next epoch (accumulate all over again)\n",
    "                magnitudes[frequency] *= 0.0\n",
    "            print(\"\\t\\tRe-aligned \",sum(re_aligned_frequencies_total)/len(re_aligned_frequencies_total),\" codewords on average per codebook\")\n",
    "    \n",
    "    print(\"*** Starting to test epoch \",epoch,\" ***\")\n",
    "    batchidx = 0\n",
    "\n",
    "    ### we track this only for curiousity - all real measurements of the paper were done afterwards, on saved images with existing libaries\n",
    "    test_psnr = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in testloader:\n",
    "            data = data.to(DEVICE)\n",
    "            out, loss_codebook, loss_commitment, indices = ae(data)\n",
    "\n",
    "            outputs, loss_codebook, loss_commitment, indices = ae(data, None)\n",
    "            \n",
    "            test_psnr += (-10.0 * torch.log10((outputs - data).square().mean(dim=3).mean(dim=2).mean(dim=1))).sum()\n",
    "\n",
    "            indices = indices[:,:,0]\n",
    "\n",
    "            #output images after every epoch\n",
    "            if batchidx == 0:\n",
    "                print(\"Examples from dataset:\")\n",
    "                save_image(torchvision.utils.make_grid(data.cpu().detach()).cpu().detach(), \"outputs/\"+SETTINGS.RUN_NAME+\"/epoch_\"+str(epoch)+\"_testset_in.png\")\n",
    "                save_image(torchvision.utils.make_grid(outputs.cpu().detach()).cpu().detach(), \"outputs/\"+SETTINGS.RUN_NAME+\"/epoch_\"+str(epoch)+\"_testset_out.png\")\n",
    "                imshow(torchvision.utils.make_grid(data.cpu().detach()).cpu().detach())\n",
    "                imshow(torchvision.utils.make_grid(outputs.cpu().detach()).cpu().detach())\n",
    "\n",
    "            batchidx += 1\n",
    "    test_psnr /= len(testloader.dataset)\n",
    "\n",
    "    losses[0] /= len(trainloader)\n",
    "    losses[1] /= len(trainloader)\n",
    "    losses[2] /= len(trainloader)\n",
    "\n",
    "    print(\"*** RESULTS FOR EPOCH \",epoch,\" ***\")\n",
    "    print(\"AVG reconstruction loss: \",losses[0])\n",
    "    print(\"AVG codebook loss: \",losses[1])\n",
    "    print(\"AVG commitment loss: \",losses[2])\n",
    "    print(\"Test PSNR: \",test_psnr.item())\n",
    "\n",
    "    print(\"_____________________________\\n\\n\\n\")\n",
    "    \n",
    "#save for later use\n",
    "torch.save(ae.state_dict(), \"outputs/trained.net\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
