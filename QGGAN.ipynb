{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>QGGAN</h1>\n",
    "Implementation according to the paper <a href=\"https://arxiv.org/abs/2407.11913\">Quantised Global Autoencoder: A Holistic Approach to Representing Visual Data</a><br/>\n",
    "This Version is meant for \"production\", i.e. uses lightning and has the option to use additional sharpening; It can do anything the other notebook can do, but is more complicated and requires more libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import lpips\n",
    "import lightning as L\n",
    "\n",
    "torch.set_printoptions(precision=8, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These settings are menat to compress ImageNet into 256 tokens, with additional VQGAN-style sharpening (QGGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    ### SETTINGS FOR QUANTISED AUTOENCODER ###\n",
    "    VQVAE_D = 64 #dimensionality in which we cluster\n",
    "    VQVAE_K = 512 #number of different codewords (i.e. number of clusters)\n",
    "    VQVAE_C = 256 #number of codewords to describe one encoded item; i.e. how many TOKENS we end up with\n",
    "    DS_SPEED_FACTOR = 3 #number of times we downscale our feature map\n",
    "    \n",
    "    ### SETTINGS FOR NETWORKS ###\n",
    "    CHANNELS_MAXIMUM = 128 #maximum number of channels in our Unets\n",
    "    CHANNELS_MINIMUM = 16 #minimum number of channels we output in Unet\n",
    "    NUM_WORKERS = 10 ###CHANGE THIS TO 0 FOR WINDOWS...\n",
    "    LR = None\n",
    "    RES_BLOCKS = 2 #number of residual blocks in sequence\n",
    "    DIM_L_EMBED = 16\n",
    "    NUM_HEADS = 128\n",
    "    USE_DROPOUT = False #set to True if you want to order the latent space hierarchically, i.e. with the first codeword containing the most, the second codeword [...]\n",
    "\n",
    "    NUM_TRAIN_BATCHES_UNTIL_RESET = 1000 #number of epochs until we reset unused codebook entries\n",
    "    #if we use more items per batch, you can reduce this\n",
    "\n",
    "    ### GAN SETTINGS ###\n",
    "    USE_GAN     = True #use VQGAN-like sharpener (\"QGGAN\")\n",
    "    WARMUP_ITS_GAN = 5000\n",
    "\n",
    "    ### RUN SETTINGS ###\n",
    "    JID = -1\n",
    "    DATASET = [\"CIFAR128\", \"CIFAR\", \"MNIST\", \"SVHN\", \"IMAGENET\", \"IMAGENET64\", \"IMAGENET128\", \"CELEB\", \"CELEB64\"][-3]\n",
    "\n",
    "    CLUSTERRUN = False\n",
    "\n",
    "    RUN_NAME = \"\" #is set later automatically\n",
    "\n",
    "    #get number of GPUs available\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    print(\"CURRENT RUN IS USING \", NUM_GPUS, \" GPUs\")\n",
    "\n",
    "    USE_CHECKPOINTS = True\n",
    "\n",
    "SETTINGS = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS.RUN_NAME = str(SETTINGS.JID)+\"_\"+SETTINGS.DATASET+\"_v5_best\" + (\"_gan\" if SETTINGS.USE_GAN else \"\") + (\"_dropout\" if SETTINGS.USE_DROPOUT else \"\")\n",
    "print(\"RUN NAME: \",SETTINGS.RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(\"outputs/\")\n",
    "mkdir(\"outputs/\"+SETTINGS.RUN_NAME+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_svhn():\n",
    "    print(\"Find this error & update all the paths below accordingly!\")\n",
    "    assert(False)\n",
    "\n",
    "    import scipy.io\n",
    "    mat = scipy.io.loadmat('/clusterarchive/ImageDatasets/SVHN/train_32x32.mat')\n",
    "    data_train = mat['X']\n",
    "    data_train = np.moveaxis(data_train, -1, 0)\n",
    "    data_train = data_train/255.0\n",
    "\n",
    "    mat = scipy.io.loadmat('/clusterarchive/ImageDatasets/SVHN/train_32x32.mat')\n",
    "    data_test = mat['X']\n",
    "    data_test = np.moveaxis(data_test, -1, 0)\n",
    "    data_test = data_test/255.0\n",
    "    return torch.tensor(data_train).permute(0, 3, 1, 2).float(), torch.tensor(data_test).permute(0, 3, 1, 2).float()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.labels = torch.zeros(len(data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTINGS.DATASET == \"CIFAR\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=SETTINGS.NUM_WORKERS)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=SETTINGS.NUM_WORKERS)\n",
    "elif SETTINGS.DATASET == \"CIFAR128\":\n",
    "    #upscaled CIFAR, useful for testing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=SETTINGS.NUM_WORKERS)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=SETTINGS.NUM_WORKERS)\n",
    "elif SETTINGS.DATASET == \"MNIST\":\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=SETTINGS.NUM_WORKERS)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=SETTINGS.NUM_WORKERS)\n",
    "elif SETTINGS.DATASET == \"SVHN\":\n",
    "    svhn_train, svhn_test = load_svhn()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    trainset = CustomDataset(svhn_train)\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=SETTINGS.NUM_WORKERS)\n",
    "    testset = CustomDataset(svhn_test)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=SETTINGS.NUM_WORKERS)\n",
    "elif SETTINGS.DATASET == \"IMAGENET\":\n",
    "    print(\"Loading ImageNet...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    print(\"Find this error & update all the paths below accordingly!\")\n",
    "    assert(False)\n",
    "\n",
    "    imagenet_data = ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/',  transform=transform)#ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/', transform=transform)\n",
    "    imagenet_data_test = ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/val/',  transform=transform)#ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/', transform=transform)\n",
    "\n",
    "    bsize = 4\n",
    "    \n",
    "    trainloader = DataLoader(imagenet_data, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    testloader = DataLoader(imagenet_data_test, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    print(\"Done loading ImageNet!\")\n",
    "\n",
    "elif SETTINGS.DATASET == \"IMAGENET64\":\n",
    "    print(\"Loading ImageNet64...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    print(\"Find this error & update all the paths below accordingly!\")\n",
    "    assert(False)\n",
    "\n",
    "    imagenet_data = ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/',  transform=transform)#ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/', transform=transform)\n",
    "    imagenet_data_test = ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/val/',  transform=transform)#ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/', transform=transform)\n",
    "\n",
    "    bsize = 4*3\n",
    "    \n",
    "    trainloader = DataLoader(imagenet_data, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    testloader = DataLoader(imagenet_data_test, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    print(\"Done loading ImageNet!\")\n",
    "elif SETTINGS.DATASET == \"IMAGENET128\":\n",
    "    print(\"Loading ImageNet...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    print(\"Find this error & update all the paths below accordingly!\")\n",
    "    assert(False)\n",
    "\n",
    "    imagenet_data = ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/',  transform=transform)#ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/', transform=transform)\n",
    "    imagenet_data_test = ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/val/',  transform=transform)#ImageFolder('/clusterarchive/ImageDatasets/imagenet/images/train/', transform=transform)\n",
    "\n",
    "    bsize = 16\n",
    "    \n",
    "    trainloader = DataLoader(imagenet_data, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    testloader = DataLoader(imagenet_data_test, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    print(\"Done loading ImageNet!\")\n",
    "\n",
    "elif SETTINGS.DATASET == \"CELEB\":\n",
    "    from PIL import Image\n",
    "    class EmbedInBlackBox(object):\n",
    "        def __init__(self, size=256):\n",
    "            self.size = size\n",
    "\n",
    "        def __call__(self, img):\n",
    "            # Ensure img is a PIL Image\n",
    "            if not isinstance(img, Image.Image):\n",
    "                raise TypeError(\"Input should be a PIL Image\")\n",
    "\n",
    "            # Resize image while maintaining aspect ratio\n",
    "            img.thumbnail((self.size, self.size))\n",
    "\n",
    "            # Create a new black image\n",
    "            new_img = Image.new(\"RGB\", (self.size, self.size), (0, 0, 0))\n",
    "\n",
    "            # Get dimensions\n",
    "            width, height = img.size\n",
    "            new_width = (self.size - width) // 2\n",
    "            new_height = (self.size - height) // 2\n",
    "\n",
    "            # Paste the original image onto the center of the black image\n",
    "            new_img.paste(img, (new_width, new_height))\n",
    "\n",
    "            return new_img\n",
    "    \n",
    "    print(\"Find this error & update all the paths below accordingly!\")\n",
    "    assert(False)\n",
    "    \n",
    "    #celeb is a bit tricky... in our case, we separated train/test split into folders,\n",
    "    #the original format is a bit of a mess \n",
    "    pre_path = \"/clusterarchive/ImageDatasets/CelebA/\"\n",
    "        \n",
    "    transform = transforms.Compose([\n",
    "        EmbedInBlackBox(size=256),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    imagenet_data = ImageFolder(pre_path+'distributed_train/', transform=transform)\n",
    "    imagenet_data_test = ImageFolder(pre_path+'distributed_test/', transform=transform)\n",
    "    \n",
    "    bsize = 4 * 2\n",
    "    \n",
    "    trainloader = DataLoader(imagenet_data, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    testloader = DataLoader(imagenet_data_test, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "elif SETTINGS.DATASET == \"CELEB64\":\n",
    "    \n",
    "    print(\"Find this error & update all the paths below accordingly!\")\n",
    "    assert(False)\n",
    "    \n",
    "    #celeb is a bit tricky... in our case, we separated train/test split into folders,\n",
    "    #the original format is a bit of a mess \n",
    "    from PIL import Image\n",
    "    pre_path = \"/clusterarchive/ImageDatasets/CelebA/\"\n",
    "        \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    imagenet_data = ImageFolder(pre_path+'distributed_train/', transform=transform)\n",
    "    imagenet_data_test = ImageFolder(pre_path+'distributed_test/', transform=transform)\n",
    "    \n",
    "    bsize = 4*3\n",
    "    \n",
    "    trainloader = DataLoader(imagenet_data, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "    testloader = DataLoader(imagenet_data_test, batch_size=bsize, shuffle=True, num_workers=SETTINGS.NUM_WORKERS, pin_memory=False)\n",
    "else:\n",
    "    print(\"INVALID DATASET CHOSEN!\")\n",
    "    assert(False)\n",
    "\n",
    "def imshow(img):\n",
    "    #check if is in interactive session:\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        npimg = img.clamp(0.0, 1.0).numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "def save_image(img, name):\n",
    "    torchvision.utils.save_image(img, name)\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    print(\"IMAGE VALUE RANGES: \",images.min(), \" to \",images.max())\n",
    "    print(\"IMAGE SHAPE: \",images.shape)\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    save_image(torchvision.utils.make_grid(images), \"outputs/\"+SETTINGS.RUN_NAME+\"/test.png\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x, DIM_L_EMBED):\n",
    "    rets = []\n",
    "    for i in range(DIM_L_EMBED):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            rets.append(fn((2. ** i) * x))\n",
    "    return torch.cat(rets, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(SETTINGS.VQVAE_C % SETTINGS.NUM_HEADS == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS.TRAIN_BATCHES = len(trainloader) / SETTINGS.NUM_GPUS\n",
    "SETTINGS.TEST_BATCHES = len(testloader) / SETTINGS.NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in trainloader:\n",
    "    SETTINGS.INPUT_C = images.size()[1]\n",
    "    SETTINGS.INPUT_W = images.size()[2]\n",
    "    break\n",
    "SETTINGS.INPUT_H = SETTINGS.INPUT_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallResidualBlock(nn.Module):\n",
    "    #this is basically the \"Imagen\" residual block\n",
    "    def __init__(self, c_in, c_int, c_out, res_dims, relu_slope=0.01):\n",
    "        super(SmallResidualBlock, self).__init__()\n",
    "\n",
    "        if True:\n",
    "            n1_no_grp = 16\n",
    "            n2_no_grp = 16\n",
    "            while (c_in) % n1_no_grp != 0:\n",
    "                n1_no_grp -= 1\n",
    "            while (c_int) % n2_no_grp != 0:\n",
    "                n2_no_grp -= 1\n",
    "            assert(n1_no_grp > 0 and n2_no_grp > 0)\n",
    "            self.norm1 = nn.GroupNorm(num_groups=n1_no_grp, num_channels=c_in, eps=1e-05, affine=True)\n",
    "            self.norm2 = nn.GroupNorm(num_groups=n2_no_grp, num_channels=c_int, eps=1e-05, affine=True)\n",
    "        else:\n",
    "            self.norm1 = nn.BatchNorm2d(num_features=c_in)\n",
    "            self.norm2 = nn.BatchNorm2d(num_features=c_int)\n",
    "        \n",
    "        self.conv_res = nn.Conv2d(kernel_size=1, in_channels=c_in, out_channels=c_out, stride=1, padding=0, bias=False)\n",
    "        self.conv_res_conditional = nn.Conv2d(kernel_size=1, in_channels=res_dims, out_channels=c_in, stride=1, padding=0, bias=False)\n",
    "        self.conv1 = nn.Conv2d(kernel_size=3, in_channels=c_in, out_channels=c_int, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(kernel_size=3, in_channels=c_int, out_channels=c_out, stride=1, padding=1)\n",
    "        \n",
    "        self.relu = nn.LeakyReLU(negative_slope=relu_slope)\n",
    "        self.activation = nn.SiLU(inplace=False)\n",
    "\n",
    "    def forward(self, x, x_conditional_res):\n",
    "        x_res = self.conv_res(x)\n",
    "\n",
    "        x = x + self.conv_res_conditional(x_conditional_res)\n",
    "        #according to Google's Imagen:\n",
    "        x = self.activation(self.norm1(x))\n",
    "        x = self.activation(self.norm2(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        x = x + x_res\n",
    "        return x\n",
    "    \n",
    "class ResidualBlocks(nn.Module):\n",
    "    def __init__(self, c_in, c_int, c_out, res_dims, relu_slope=0.01, RES_BLOCKS=3):\n",
    "        super(ResidualBlocks, self).__init__()\n",
    "\n",
    "        #list of small residual blocks #\n",
    "        self.blocks = nn.ModuleList()\n",
    "        # use exactly THREE res blocks!\n",
    "        for i in range(0, RES_BLOCKS):\n",
    "            cur_c_in, cur_c_out = c_int, c_int\n",
    "            if i == 0:\n",
    "                cur_c_in = c_in\n",
    "            if i == RES_BLOCKS - 1:\n",
    "                cur_c_out = c_out\n",
    "            self.blocks.append(SmallResidualBlock(cur_c_in, c_int, cur_c_out, res_dims=res_dims, relu_slope=relu_slope))\n",
    "\n",
    "    def forward(self, x, x_conditional_res):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, x_conditional_res)\n",
    "        return x\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in, c_int, width_height, NO_CHANNELS_MAX, NO_CHANNELS_MIN, res_dims, relu_slope=0.01, current_layer=0, skip_up_layers=0, skip_down_layers=0, target_out=None):\n",
    "        super(UNetBlock, self).__init__()\n",
    "\n",
    "        self.in_conv = nn.Conv2d(kernel_size=3, in_channels=c_in, out_channels=c_int, stride=1, padding=1)\n",
    "        c_next = min(c_int * 2, NO_CHANNELS_MAX)\n",
    "        self.in_res = ResidualBlocks(c_int, c_int, c_out=c_next, res_dims=res_dims, relu_slope=relu_slope, RES_BLOCKS=SETTINGS.RES_BLOCKS)\n",
    "\n",
    "        self.width_height = width_height\n",
    "\n",
    "        if self.width_height > 8 or (skip_up_layers > current_layer or current_layer < skip_down_layers): #don't go below 8x8, that makes no sense (except if we have to)\n",
    "            self.unet = UNetBlock(SETTINGS, c_in=c_next, c_int=c_next, width_height=int(width_height/2), NO_CHANNELS_MAX=NO_CHANNELS_MAX, NO_CHANNELS_MIN=NO_CHANNELS_MIN, res_dims=res_dims, relu_slope=relu_slope, current_layer=current_layer+1, skip_up_layers=skip_up_layers, skip_down_layers=skip_down_layers)\n",
    "            self.downsample = nn.Upsample(scale_factor=0.5, mode='bilinear')\n",
    "            self.downsample_cond = nn.Upsample(scale_factor=0.5, mode='nearest')\n",
    "            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "            next_input = self.unet.out_conv.out_channels\n",
    "        else:\n",
    "            self.unet = None\n",
    "            next_input = c_next\n",
    "\n",
    "        dim_out = c_int\n",
    "        if skip_up_layers > current_layer:\n",
    "            dim_out = self.unet.out_conv.out_channels\n",
    "        \n",
    "        if current_layer == 0 and target_out != None:\n",
    "            dim_out = target_out\n",
    "        \n",
    "        self.out_res = ResidualBlocks(next_input, max(c_int, dim_out), c_out=dim_out, res_dims=res_dims, relu_slope=relu_slope, RES_BLOCKS=SETTINGS.RES_BLOCKS)\n",
    "        self.out_conv = nn.Conv2d(kernel_size=3, in_channels=dim_out, out_channels=dim_out, stride=1, padding=1)\n",
    "        self.full_residual = nn.Conv2d(kernel_size=1, in_channels=c_in, out_channels=dim_out, stride=1, padding=0)\n",
    "        \n",
    "        if current_layer < skip_down_layers:\n",
    "            #make sure we do SHRINK the number of parameters!\n",
    "            if self.unet == None:\n",
    "                prev_ch = c_next\n",
    "            else:\n",
    "                prev_ch = int(self.unet.out_conv.out_channels)\n",
    "            layers_to_go = min(NO_CHANNELS_MIN * (2 ** current_layer), prev_ch)\n",
    "            if current_layer == 0 and target_out != None:\n",
    "                layers_to_go = target_out\n",
    "            \n",
    "            self.out_res = ResidualBlocks(next_input, next_input, c_out=layers_to_go, res_dims=res_dims, relu_slope=relu_slope, RES_BLOCKS=SETTINGS.RES_BLOCKS)\n",
    "            self.out_conv = nn.Conv2d(kernel_size=3, in_channels=layers_to_go, out_channels=layers_to_go, stride=1, padding=1)\n",
    "            self.full_residual = nn.Conv2d(kernel_size=1, in_channels=c_in, out_channels=layers_to_go, stride=1, padding=0)\n",
    "        \n",
    "        self.activation = nn.SiLU(inplace=False)\n",
    "\n",
    "        self.current_layer = current_layer\n",
    "        self.skip_up_layers = skip_up_layers\n",
    "        self.skip_down_layers = skip_down_layers\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        #input block\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        ###print(\"\\t\" * self.current_layer, \"ENCODER - CURRENT SIZE: \", x.size()[2], \"x\", x.size()[3], \" with \", x.size()[1] , \" channels\")\n",
    "        #apply (multiple?) residual blocks\n",
    "        x = self.in_res(x, residual)\n",
    "        if x.size()[2] > 1 and self.unet != None:\n",
    "            #downscale\n",
    "            if self.skip_down_layers <= self.current_layer:\n",
    "                x = self.downsample(x)\n",
    "            ds_res = residual\n",
    "            while x.size()[2] < ds_res.size()[2]:\n",
    "                ds_res = self.downsample_cond(ds_res)\n",
    "            #recursive\n",
    "            x = self.unet(x, ds_res)\n",
    "            #upscale\n",
    "            if self.skip_up_layers <= self.current_layer:\n",
    "                x = self.upsample(x)\n",
    "                \n",
    "        #output residual\n",
    "        while x.size()[2] > residual.size()[2]:\n",
    "            residual = self.upsample(residual)\n",
    "        while x.size()[2] < residual.size()[2]:\n",
    "            residual = self.downsample(residual)\n",
    "\n",
    "        ###print(\"\\t\" * self.current_layer, \"DECODER - CURRENT SIZE: \", x.size()[2], \"x\", x.size()[3], \" with \", x.size()[1] , \" channels\")\n",
    "        x = self.out_res(x, residual)\n",
    "        #convolution\n",
    "        x = self.out_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in, c_int, c_out, NO_CHANNELS_MAX, NO_CHANNELS_MIN, width_height, skip_up_layers=0, skip_down_layers=0):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        assert(skip_up_layers == 0 or skip_down_layers == 0)\n",
    "\n",
    "        self.SETTINGS = SETTINGS\n",
    "        res_dims = 2 * 2 * self.SETTINGS.DIM_L_EMBED + c_in\n",
    "        self.position = nn.ParameterList()\n",
    "        wh = width_height\n",
    "        self.wh = width_height\n",
    "        while wh >= 1:\n",
    "            self.position.append(nn.Parameter(self.grid_positional_encoding(wh)[None], requires_grad=False))\n",
    "            wh = int(wh / 2)\n",
    "        \n",
    "        self.unet_blocks = UNetBlock(SETTINGS, c_in, c_int, width_height=width_height, NO_CHANNELS_MAX=NO_CHANNELS_MAX, NO_CHANNELS_MIN=NO_CHANNELS_MIN, res_dims=res_dims, skip_up_layers=skip_up_layers, skip_down_layers=skip_down_layers, target_out=c_out)\n",
    "\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode='bilinear')\n",
    "        self.upsample = nn.Upsample(scale_factor=2.0, mode='bilinear')\n",
    "        \n",
    "        self.mega_residual = torch.nn.Conv2d(kernel_size=3, in_channels=c_in, out_channels=self.unet_blocks.out_conv.out_channels, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.out_projection = nn.Conv2d(kernel_size=3, in_channels=self.unet_blocks.out_conv.out_channels, out_channels=c_out, stride=1, padding=1, bias=False)\n",
    "        \n",
    "    def grid_positional_encoding(self, width_height):\n",
    "        grid = torch.ones(width_height, width_height, 2)\n",
    "        for x in range(0, width_height):\n",
    "            grid[x,:,0] = x / width_height\n",
    "        for y in range(0, width_height):\n",
    "            grid[:,y,1] = y / width_height\n",
    "        #first dimension is what we should encode\n",
    "        rets = []\n",
    "        for i in range(self.SETTINGS.DIM_L_EMBED):\n",
    "            for fn in [torch.sin, torch.cos]:\n",
    "                rets.append(fn((2. ** i) * grid))\n",
    "        return torch.cat(rets, -1).transpose(2,1).transpose(1,0)\n",
    "\n",
    "    #decode in the sense of: turn into embeddings\n",
    "    def decode_indices(self, indices, codebook):\n",
    "        w, h = indices.size()[1], indices.size()[2]\n",
    "        output = codebook.transpose(0,1)[indices.view(-1)]\n",
    "        output = output.transpose(0,1)\n",
    "        output = output.view(output.size()[0], indices.size()[0], w, h).transpose(0,1)\n",
    "        return output\n",
    "    \n",
    "    def get_positions(self, width_height):\n",
    "        positions = []\n",
    "        cur_wh = width_height\n",
    "        while cur_wh >= 1:\n",
    "            ### use only relative position\n",
    "            diff = self.wh - cur_wh\n",
    "            if diff == 0:\n",
    "                index = 0\n",
    "            else:\n",
    "                index = int(math.log2(self.wh)) - int(math.log2(cur_wh)) #0 at first\n",
    "            position_absolute = self.position[index].clone()\n",
    "            cur_wh = int(cur_wh / 2)\n",
    "            positions.append(position_absolute)\n",
    "        return positions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = torch.cat((x, self.position[0].repeat(x.size()[0], 1, 1, 1)), 1)\n",
    "        mega_res = self.mega_residual(x)\n",
    "        #downscale:\n",
    "        out = self.unet_blocks(x, residual)\n",
    "\n",
    "        while out.size()[2] > mega_res.size()[2]:\n",
    "            mega_res = self.upsample(mega_res)\n",
    "        while out.size()[2] < mega_res.size()[2]:\n",
    "            mega_res = self.downsample(mega_res)\n",
    "        result = self.out_projection(out + mega_res)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode='bilinear')\n",
    "\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=c_in, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(self.downsample)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(self.downsample)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.VQVAE_C, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, c_out):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2.0, mode='bilinear')\n",
    "\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.VQVAE_C, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(self.upsample)\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=SETTINGS.CHANNELS_MAXIMUM, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(self.upsample)\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=SETTINGS.CHANNELS_MAXIMUM, out_channels=c_out, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu) #only because we have the output values in [0, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in, c_int, wh):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.block_in = SmallResidualBlock(c_in, c_int, c_int, res_dims=c_in, relu_slope=0.2)\n",
    "\n",
    "        self.unet_blocks = UNetBlock(SETTINGS, c_int, c_int, width_height=wh, NO_CHANNELS_MAX=SETTINGS.CHANNELS_MAXIMUM, NO_CHANNELS_MIN=16, res_dims=c_in, relu_slope=0.2)\n",
    "        \n",
    "        self.block_out = SmallResidualBlock(c_int, c_int, 1, res_dims=c_in, relu_slope=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        #in\n",
    "        x = self.block_in(x, x_input)\n",
    "        #unet\n",
    "        x = self.unet_blocks(x, x_input)\n",
    "        #residual after input layer\n",
    "        x = self.block_out(x, x_input)\n",
    "        #patchwise discriminator\n",
    "        return x\n",
    "discriminator = Discriminator(SETTINGS, SETTINGS.INPUT_C, SETTINGS.CHANNELS_MINIMUM, SETTINGS.INPUT_W)\n",
    "discriminator(torch.rand(4, 3, 128, 128))\n",
    "print(\"NO PARAMS: \", sum(p.numel() for p in discriminator.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fullsize(x):\n",
    "    if len(x.size()) == 1:\n",
    "        return \"ERROR: get_fullsize() called with 1D tensor\"\n",
    "    elif len(x.size()) == 2:\n",
    "        return x.size()[1]\n",
    "    elif len(x.size()) == 3:\n",
    "        return x.size()[1]*x.size()[2]\n",
    "    elif len(x.size()) == 4:\n",
    "        return x.size()[1]*x.size()[2]*x.size()[3]\n",
    "    elif len(x.size()) == 5:\n",
    "        return x.size()[1]*x.size()[2]*x.size()[3]*x.size()[4]\n",
    "    else:\n",
    "        print(\"ERROR: get_fullsize() called with tensor of size \", x.size())\n",
    "\n",
    "class BaseAE(nn.Module):\n",
    "    def __init__(self, SETTINGS, c_in):\n",
    "        super(BaseAE, self).__init__()\n",
    "\n",
    "        self.SETTINGS = SETTINGS\n",
    "        self.relu = torch.nn.LeakyReLU() #we use leaky relu as activation function, as it also has a gradient for an input < 0\n",
    "        self.reduce = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.upscale = torch.nn.Upsample(scale_factor=2.0, mode='bilinear')\n",
    "\n",
    "        self.residual_encoder = torch.nn.Conv2d(kernel_size=1, in_channels=SETTINGS.INPUT_C, out_channels=SETTINGS.VQVAE_C, stride=1, padding=0)\n",
    "        self.residual_decoder = torch.nn.Conv2d(kernel_size=1, in_channels=SETTINGS.VQVAE_C, out_channels=SETTINGS.INPUT_C, stride=1, padding=0)\n",
    "\n",
    "        if True:\n",
    "            self.encoder = UNet(SETTINGS, c_in=c_in, c_int=SETTINGS.CHANNELS_MINIMUM, c_out=SETTINGS.VQVAE_C, NO_CHANNELS_MAX=SETTINGS.CHANNELS_MAXIMUM, NO_CHANNELS_MIN=SETTINGS.CHANNELS_MINIMUM, width_height=SETTINGS.INPUT_W, skip_up_layers=SETTINGS.DS_SPEED_FACTOR)\n",
    "            \n",
    "            #create a list of projections:\n",
    "            self.down_proj = nn.ModuleList()\n",
    "            self.up_proj   = nn.ModuleList()\n",
    "            for i in range(0, SETTINGS.NUM_HEADS):\n",
    "                self.down_proj.append(torch.nn.Conv1d(in_channels=int(SETTINGS.INPUT_W*SETTINGS.INPUT_H/(2 ** SETTINGS.DS_SPEED_FACTOR)/(2 ** SETTINGS.DS_SPEED_FACTOR)), out_channels=SETTINGS.VQVAE_D, kernel_size=1, stride=1, padding=0))\n",
    "                self.up_proj.append(torch.nn.Conv1d(in_channels=SETTINGS.VQVAE_D, out_channels=int(SETTINGS.INPUT_W*SETTINGS.INPUT_H/(2 ** SETTINGS.DS_SPEED_FACTOR)/(2 ** SETTINGS.DS_SPEED_FACTOR)), kernel_size=1, stride=1, padding=0))\n",
    "            self.decoder = UNet(SETTINGS, c_in=SETTINGS.VQVAE_C, c_int=SETTINGS.CHANNELS_MAXIMUM, c_out=c_in, NO_CHANNELS_MAX=SETTINGS.CHANNELS_MAXIMUM, NO_CHANNELS_MIN=SETTINGS.CHANNELS_MINIMUM, width_height=int(SETTINGS.INPUT_W/(2 ** SETTINGS.DS_SPEED_FACTOR)), skip_down_layers=SETTINGS.DS_SPEED_FACTOR)\n",
    "        \n",
    "        self.codebook = torch.nn.Parameter((torch.rand(SETTINGS.VQVAE_C, SETTINGS.VQVAE_K, SETTINGS.VQVAE_D) * 2.0 - 1.0) * 0.5, requires_grad=True)\n",
    "        self.offset = torch.nn.Parameter((torch.rand(SETTINGS.VQVAE_D, SETTINGS.VQVAE_C) * 2.0 - 1.0) * 0.5, requires_grad=True)\n",
    "\n",
    "        self.pos = positional_encoding(torch.arange(0, SETTINGS.VQVAE_C)[:,None] / (SETTINGS.VQVAE_C+1), 10)[None,:,:].transpose(1,2)\n",
    "\n",
    "    def quantise_individually(self, x):\n",
    "        x_before_rounding = x.clone()\n",
    "        #size  x in: [b x D x C x 1]\n",
    "        #size cb in: [C x K x D]\n",
    "        x_before_rounding = x.clone()\n",
    "\n",
    "        ### torch.rand(C, D, K)\n",
    "        B = x.size()[0]\n",
    "        x = x.transpose(1,2)[:,:,:,0]\n",
    "        codebook = self.codebook.transpose(1,2)\n",
    "\n",
    "        dists = (x[..., None] - codebook[None]).square().sum(-2)#\n",
    "        idx = dists.argmin(-1)\n",
    "        \n",
    "        codebook_expanded = codebook[None].expand(B, -1, -1, -1)\n",
    "        idx_expanded = idx[..., None, None].expand(-1, -1, self.SETTINGS.VQVAE_D, -1)\n",
    "        \n",
    "        x_rounded = torch.gather(codebook_expanded, -1, idx_expanded).transpose(1,2)\n",
    "\n",
    "        loss_commitment = (x_before_rounding - x_rounded.detach()).square().mean() #only have gradient for encoder \n",
    "        loss_codebook   = (x_before_rounding.detach() - x_rounded).square().mean() #only have gradient for codebook\n",
    "\n",
    "        x = x_before_rounding - (x_before_rounding - x_rounded).detach()\n",
    "        \n",
    "        return x, loss_codebook, loss_commitment, idx.view(x.size()[0], x.size()[2], x.size()[3])\n",
    "    \n",
    "    def quantise(self, x):\n",
    "        #this is where the magic happens:\n",
    "        #    1. take input of size [b x W x H x VQVAE_D] and reshape to [b x VQVAE_D x W x H ];\n",
    "        #       this means we have ALL the codewords in the last dimension now, so we can just merge together all other dimensions to easily compare them to the codewords\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        size_in = x.size()\n",
    "        x = x.view(-1, self.SETTINGS.VQVAE_D)\n",
    "        #   2. compare [b*W*H x VQVAE_D] to [CODEWORDS x VQVAE_D]: by re-shaping to have [b*W*H x 1 x VQVAE_D] and [1 x CODEWORDS x VQVAE_D], the substraction will\n",
    "        #      give us a [b*W*H x CODEWORDS X VQVAE_D] tensor of all distance pairs between input elements (b*W*H) and the CODEWORDS-many codewords; just square, \n",
    "        #      then sum up the last dimension to get sum of the distance values for each dimension (=euclidean squared distance)\n",
    "        #      and then take the argmin to get the index of the closest codeword for each input element - these are our indices we want!\n",
    "        if self.SETTINGS.DATASET != \"IMAGENET\":\n",
    "            with torch.no_grad():\n",
    "                indices = (x[:,None] - self.codebook.view(1, self.SETTINGS.VQVAE_K, self.SETTINGS.VQVAE_D)).square().sum(dim=2).argmin(dim=1)\n",
    "        else:\n",
    "            #baseline:\n",
    "            chunksize = int(x.size()[0]/8)#1024 # baseline value divided by number of elements to tokenise\n",
    "            with torch.no_grad():\n",
    "                index_begin = 0\n",
    "                index_end   = chunksize\n",
    "                acc_indices = []\n",
    "                while index_begin < x.size()[0]:\n",
    "                    indices = (x[index_begin:index_end][:,None] - self.codebook.view(1, self.SETTINGS.VQVAE_K, self.SETTINGS.VQVAE_D)).square().sum(dim=2).argmin(dim=1)\n",
    "                    acc_indices.append(indices)\n",
    "                    index_begin += chunksize\n",
    "                    index_end += chunksize\n",
    "                indices = torch.cat(acc_indices)\n",
    "            \n",
    "        #   3. we take those indices we just looked up: these are the indices of the closest codeword for each input element, so we can now look up the actual \n",
    "        #      codeword to find the closest codeword to each input element aka the \"centroids\" we round to\n",
    "        x_rounded = self.codebook[indices]\n",
    "        \n",
    "        #   4. we calculate the loss for the commitment loss (i.e. does the encoder produce stuff from the codebook?) and the codebook loss (how close is the codebook to the encoder outputs?)\n",
    "        #      the value is exactly the same for both terms, but this way we can a) train encoder and codebook with different magnitudes and b) the encoder outputs and codebook outputs are\n",
    "        #      not just \"shrinked\" to one super tiny value: the loss we be almost zero if codebook and encoder outputs would be just downscaled by some tiny factor (latent space just shrinks together)\n",
    "        #      to avoid that, we use the detach() function, making sure that codebook and encoder outputs move individually and not just shrink together to something tiny\n",
    "        loss_commitment = (x - x_rounded.detach()).square().mean() #only have gradient for encoder \n",
    "        loss_codebook   = (x.detach() - x_rounded).square().mean() #only have gradient for codebook\n",
    "\n",
    "        #   5. the actual rounding we do: we take the difference between the rounded and the unrounded value and add it to the unrounded value\n",
    "        #      the paper calls this \"straight through estimator\", as we just pass the gradient through the rounding operation (which is not differentiable)\n",
    "        #      what essentially happens here is that we just take our x and substract some float without any gradient, similar to writing \"x = x - 0.1\":\n",
    "        #      \"(x - x_rounded).detach()\" becomes just some number that we substract from x; this converts x to the same values as x_rounded, but the gradient is not passed through this operation\n",
    "        #      i.e. we round, but keep the (then a bit inexact) gradient \n",
    "        x = x - (x - x_rounded).detach() #change x to x_rounded, but keep the gradient from x\n",
    "        #   6. we reshape back to the original shape and return the values; we also return both losses and the indices of the closest codewords\n",
    "        x = x.view(size_in)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x, loss_codebook, loss_commitment, indices.view(x.size()[0], x.size()[2], x.size()[3])\n",
    "    \n",
    "    def encode(self, x):\n",
    "        #1. encode\n",
    "        res = self.residual_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        while x.size()[2] < res.size()[2]:\n",
    "            res = self.reduce(res)\n",
    "        x = x + res #self.upscale\n",
    "        #2. re-shape and swap dimensions\n",
    "        x = x.view(x.size()[0], self.SETTINGS.VQVAE_C, -1)\n",
    "        x = x.transpose(1,2)\n",
    "        #3. project down\n",
    "        w = x.size()[2]\n",
    "        x_out = None\n",
    "        for i in range(0, SETTINGS.NUM_HEADS):\n",
    "            out = self.down_proj[i](x[:,:,int(w/SETTINGS.NUM_HEADS*(i)):int(w/SETTINGS.NUM_HEADS*(i+1))])\n",
    "            if x_out == None:\n",
    "                x_out = out\n",
    "            else:\n",
    "                x_out = torch.cat((x_out, out), 2)\n",
    "        x = x_out\n",
    "\n",
    "        #4. quantise\n",
    "        #X B4 Q:  torch.Size([4, 64, 256])\n",
    "        x = x.contiguous().view(x.size()[0], self.SETTINGS.VQVAE_D, -1, 1) #unfold, here just for quantisation\n",
    "\n",
    "        x, loss_codebook, loss_commitment, indices = self.quantise_individually(x)\n",
    "\n",
    "        #don't add individual offsets, but use the global one\n",
    "        x = x + self.offset[None, :, :, None]\n",
    "        return x, loss_codebook, loss_commitment, indices\n",
    "\n",
    "    def decode(self, x, w, h):\n",
    "        x = x.view(x.size()[0], self.SETTINGS.VQVAE_D, -1) #fold back after quantisation; REDUNDANT!!! only for using only decoder\n",
    "        #5. project up\n",
    "        x_out = None\n",
    "        blocksize = int(x.size()[2]/SETTINGS.NUM_HEADS)\n",
    "        for i in range(0, SETTINGS.NUM_HEADS):\n",
    "            out = self.up_proj[i](x[:,:,(blocksize*i):(blocksize * (i+1))])\n",
    "            if x_out == None:\n",
    "                x_out = out\n",
    "            else:\n",
    "                x_out = torch.cat((x_out, out), 2)\n",
    "        x = x_out\n",
    "        \n",
    "        #6. reshape back\n",
    "        x = x.transpose(1,2) #swap global information and local information back again (channels vs spatial dimensions)\n",
    "        x = x.contiguous().view(x.size()[0], self.SETTINGS.VQVAE_C, int(w/(2 ** self.SETTINGS.DS_SPEED_FACTOR)), int(h/(2 ** self.SETTINGS.DS_SPEED_FACTOR)))\n",
    "        #7. decode\n",
    "        res = self.residual_decoder(x)\n",
    "        x = self.decoder(x)\n",
    "        while x.size()[2] > res.size()[2]:\n",
    "            res = self.upscale(res)\n",
    "        x = x + res\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, dropout_mask=None):\n",
    "        #encode, then quantise, then decode; pass the codebook loss and commitment loss through so we can use them for training\n",
    "        w, h = x.size()[2], x.size()[3]\n",
    "        x, loss_codebook, loss_commitment, indices = self.encode(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], self.SETTINGS.VQVAE_D, -1) #fold back after quantisation\n",
    "        if dropout_mask != None:\n",
    "            x = x * dropout_mask[:,None,:]\n",
    "            \n",
    "        x = self.decode(x, w, h)\n",
    "\n",
    "        return x, loss_codebook, loss_commitment, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parallel_GQAE(L.LightningModule):\n",
    "    def backward(self, loss):\n",
    "        loss.backward()\n",
    "    \n",
    "    def __init__(self, SETTINGS, ae, discriminator=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ae = ae\n",
    "        self.SETTINGS = SETTINGS\n",
    "        self.APPLY_GAN = False\n",
    "        self.STEPS = 0\n",
    "        self.unique_indices = set()\n",
    "        \n",
    "        self.index_count = torch.zeros(self.SETTINGS.VQVAE_C, self.SETTINGS.VQVAE_K)\n",
    "        self.magnitudes = torch.zeros(self.SETTINGS.VQVAE_C, self.SETTINGS.VQVAE_K)\n",
    "        self.EPSILON = 0.000001\n",
    "        self.rec_loss = []\n",
    "        self.TOTAL_ITS = 0\n",
    "        self.last_output = None\n",
    "        self.best_test = None\n",
    "\n",
    "        self.start_epoch = time.time()\n",
    "        \n",
    "        if self.SETTINGS.USE_GAN:\n",
    "            self.automatic_optimization = False\n",
    "            self.hinge = nn.ReLU()\n",
    "            self.loss_fn_perceptual = lpips.LPIPS(net='vgg')\n",
    "            self.discriminator = discriminator\n",
    "    \n",
    "    def print(self, *args, **kwargs):\n",
    "        if self.global_rank == 0:\n",
    "            print(*args, **kwargs)\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        #track gradients of codewords to find out which ones to reset\n",
    "        self.magnitudes += self.ae.codebook.grad.abs().mean(dim=2).detach().cpu()\n",
    "        \n",
    "    def generic_step(self, batch, batchidx, test=False):\n",
    "        data, label = batch\n",
    "\n",
    "        if not test:\n",
    "            self.TOTAL_ITS += 1\n",
    "            \n",
    "        prefix = \"train\"\n",
    "        if test:\n",
    "            prefix = \"test\"\n",
    "\n",
    "        if not self.APPLY_GAN and self.SETTINGS.USE_GAN and self.TOTAL_ITS > self.SETTINGS.WARMUP_ITS_GAN:\n",
    "            self.APPLY_GAN = True\n",
    "            self.print(\"--> STARTING TO APPLY GAN\")\n",
    "        \n",
    "        if self.SETTINGS.USE_GAN:\n",
    "            optimizer_ae, optimizer_dis = self.optimizers()\n",
    "            optimizer_ae.zero_grad()\n",
    "\n",
    "        ### discriminator ###\n",
    "        if self.APPLY_GAN and batchidx % 2 == 1 and not test:\n",
    "            optimizer_dis.zero_grad()\n",
    "\n",
    "            self.ae.train(False)\n",
    "            self.discriminator.train(True)\n",
    "\n",
    "            outputs, _, _, _ = self.ae(data)\n",
    "\n",
    "            out_real = self.discriminator(data)\n",
    "            out_generated = self.discriminator(outputs)\n",
    "            \n",
    "            #real should be 1.0, generated should be -1.0\n",
    "            loss_discriminator = 0.5 * (self.hinge(1.0 - out_real).mean() + self.hinge(1.0 + out_generated).mean())\n",
    "            self.log(prefix+\"_loss_disc\", loss_discriminator.item(), on_epoch=True, sync_dist=False)\n",
    "\n",
    "            loss_discriminator.backward()\n",
    "            optimizer_dis.step()\n",
    "            \n",
    "            return\n",
    "    \n",
    "        ### generator ###\n",
    "        if True:\n",
    "            self.ae.train(True)\n",
    "            if self.SETTINGS.USE_GAN:\n",
    "                self.discriminator.train(False)\n",
    "\n",
    "            dropout_mask_binary = torch.randint(0, 3, (data.size()[0],))\n",
    "            dropout_mask = torch.ones(data.size()[0], self.SETTINGS.VQVAE_C, device=data.device)\n",
    "            if self.SETTINGS.USE_DROPOUT and not test:\n",
    "                for i in range(dropout_mask.size()[0]):\n",
    "                    if dropout_mask_binary[i] == 0:\n",
    "                        dropout_mask[i,torch.randint(1, self.SETTINGS.VQVAE_C, (1,)).item():] = 0.0\n",
    "            else:\n",
    "                dropout_mask = None\n",
    "            \n",
    "            outputs, loss_codebook, loss_commitment, indices = self.ae(data, dropout_mask)\n",
    "            \n",
    "            indices = indices[:,:,0]\n",
    "\n",
    "            if not test:\n",
    "                onehot = torch.nn.functional.one_hot(indices.detach().cpu(), num_classes=SETTINGS.VQVAE_K)\n",
    "                onehot = onehot.sum(dim=0)\n",
    "                #now shape [256 x 512]\n",
    "                self.index_count += onehot\n",
    "\n",
    "            #crop:\n",
    "            if SETTINGS.DATASET == \"CELEB\":\n",
    "                data = data[:,:,19:(19+218),39:(39+178)]\n",
    "                outputs = outputs[:,:,19:(19+218),39:(39+178)]\n",
    "\n",
    "            #output images after every epoch\n",
    "            if self.global_rank == 0 and batchidx == 0 and not test:\n",
    "                save_image(torchvision.utils.make_grid(data.cpu().detach()).cpu().detach(), \"outputs/\"+self.SETTINGS.RUN_NAME+\"/epoch_\"+str(self.current_epoch)+\"_in.png\")\n",
    "                save_image(torchvision.utils.make_grid(outputs.cpu().detach()).cpu().detach(), \"outputs/\"+self.SETTINGS.RUN_NAME+\"/epoch_\"+str(self.current_epoch)+\"_out.png\")\n",
    "                imshow(torchvision.utils.make_grid(data.cpu().detach()).cpu().detach())\n",
    "                imshow(torchvision.utils.make_grid(outputs.cpu().detach()).cpu().detach())\n",
    "\n",
    "            #count unique indices:\n",
    "            self.unique_indices.update(torch.unique(indices).tolist())\n",
    "            #make sure unique indices stay unique\n",
    "            self.unique_indices = set(self.unique_indices)\n",
    "            \n",
    "            if self.APPLY_GAN:\n",
    "                #use perceptual loss + l1 loss\n",
    "                loss_reconstruction = self.loss_fn_perceptual(outputs, data).mean()\n",
    "                loss_l1 = (outputs - data).abs().mean()\n",
    "            else:\n",
    "                #MSE\n",
    "                loss_reconstruction = (outputs - data).square()\n",
    "                if test:\n",
    "                    loss_reconstruction = (outputs - data).square()\n",
    "                    loss_reconstruction = (-10.0 * torch.log10(loss_reconstruction.mean(dim=3).mean(dim=2).mean(dim=1))).mean()\n",
    "                loss_reconstruction = loss_reconstruction.mean()\n",
    "            \n",
    "            if self.APPLY_GAN and not test:\n",
    "                loss_gan = -(self.discriminator(outputs)).mean()\n",
    "\n",
    "                loss_rec_grads = torch.autograd.grad(loss_reconstruction, self.ae.decoder.unet_blocks.out_conv.weight, retain_graph=True)[0]\n",
    "                loss_gan_grads = torch.autograd.grad(loss_gan, self.ae.decoder.unet_blocks.out_conv.weight, retain_graph=True)[0]\n",
    "                gan_lambda = 0.75 * torch.norm(loss_rec_grads) / torch.norm(loss_gan_grads + self.EPSILON)\n",
    "                gan_lambda = gan_lambda.detach()\n",
    "                \n",
    "                loss_reconstruction += loss_l1\n",
    "            elif self.APPLY_GAN:\n",
    "                gan_lambda = 0.0\n",
    "                loss_gan = torch.zeros(1, device=data.device)\n",
    "\n",
    "            loss_non_rec = (loss_codebook * 0.25 + loss_commitment)\n",
    "            loss = loss_reconstruction + loss_non_rec\n",
    "            if self.APPLY_GAN:\n",
    "                loss += gan_lambda * loss_gan\n",
    "                self.log(prefix+\"_loss_gan\", gan_lambda * loss_gan.item(), on_epoch=True, sync_dist=False)\n",
    "\n",
    "            self.log(prefix+\"_loss_rec\", loss_reconstruction.item(), on_epoch=True, sync_dist=False)\n",
    "            self.log(prefix+\"_loss_code\", loss_codebook.item(), on_epoch=True, sync_dist=False)\n",
    "            self.log(prefix+\"_loss_comm\", loss_commitment.item(), on_epoch=True, sync_dist=False)\n",
    "            self.rec_loss.append(loss_reconstruction.item())\n",
    "            self.rec_loss = self.rec_loss[-100:]\n",
    "            \n",
    "            if not test:\n",
    "                if self.TOTAL_ITS == 100 and self.global_rank == 0:\n",
    "                    time_taken = time.time() - self.epoch_start_time\n",
    "                    time_per_it = time_taken / self.TOTAL_ITS\n",
    "                    self.print(\"Projected total time for this epoch: \",((self.SETTINGS.TRAIN_BATCHES+self.SETTINGS.TEST_BATCHES) * time_per_it),\" seconds\")\n",
    "            \n",
    "                if self.global_rank == 0 and (self.last_output == None or (time.time() > self.last_output + 20.0)) and batchidx > 100:\n",
    "                    self.print(\"\\t\"+str(batchidx/self.SETTINGS.TRAIN_BATCHES*100)+\"% done, current running mean: \"+str(sum(self.rec_loss)/len(self.rec_loss)))\n",
    "                    self.print(\"\\t--> \"+str((time.time() - self.epoch_start_time) / (batchidx/self.SETTINGS.TRAIN_BATCHES) - (time.time() - self.epoch_start_time))+\" seconds left until epoch is concluded\")\n",
    "                    self.last_output = time.time()\n",
    "            \n",
    "            if self.SETTINGS.USE_GAN and not test:\n",
    "                loss.backward()\n",
    "                self.magnitudes += self.ae.codebook.grad.abs().mean(dim=2).detach().cpu()\n",
    "                optimizer_ae.step()\n",
    "                return\n",
    "\n",
    "            return loss\n",
    "\n",
    "    def validation_step(self, loaded_data, batchidx):\n",
    "        return self.generic_step(loaded_data, batchidx, test=True)\n",
    "        \n",
    "    def training_step(self, loaded_data, batchidx):\n",
    "        #re-initialise the codebooks:\n",
    "        if self.SETTINGS.NUM_TRAIN_BATCHES_UNTIL_RESET != None and self.TOTAL_ITS % self.SETTINGS.NUM_TRAIN_BATCHES_UNTIL_RESET == 0 and self.TOTAL_ITS > 0:\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"---> RE-ALIGNING FREQUENCIES <---\")\n",
    "            print(\"---------------------------------\")\n",
    "            \n",
    "            re_aligned_frequencies_total = []\n",
    "            for frequency in range(0, self.SETTINGS.VQVAE_C):\n",
    "                #1. while there exist codewords with magnitude = 0, re-initialise them\n",
    "                re_aligned_frequencies = 0\n",
    "                while True:\n",
    "                    #find a codeword to re-distribute\n",
    "                    unused_codeword = self.magnitudes[frequency].argmin()\n",
    "                    if self.magnitudes[frequency, unused_codeword] > 0:\n",
    "                        break\n",
    "\n",
    "                    #find a codeword to split up\n",
    "                    replacement_codeword = self.magnitudes[frequency].argmax()\n",
    "                    if self.magnitudes[frequency, replacement_codeword] <= self.EPSILON:\n",
    "                        break\n",
    "\n",
    "                    ### torch.zeros(self.SETTINGS.VQVAE_C, self.SETTINGS.VQVAE_K, self.SETTINGS.VQVAE_D)\n",
    "                    #re-distribute the codeword: find the direction with the highest magnitude and move the unused codeword in that direction\n",
    "                    direction = (torch.rand(self.SETTINGS.VQVAE_D) * 2.0 - 1.0) * 0.0001\n",
    "                    with torch.no_grad():\n",
    "                        self.ae.codebook[frequency, unused_codeword] = self.ae.codebook[frequency, replacement_codeword].to(self.ae.codebook[frequency, unused_codeword].get_device()) + direction.to(self.ae.codebook[frequency, unused_codeword].get_device())\n",
    "\n",
    "                    #set the replacement codeword/unused codeword to EPSILON to make sure that we don't use it again\n",
    "                    self.magnitudes[frequency, replacement_codeword] = self.EPSILON\n",
    "                    self.magnitudes[frequency, unused_codeword]      = self.EPSILON\n",
    "                    re_aligned_frequencies += 1\n",
    "                re_aligned_frequencies_total.append(re_aligned_frequencies)\n",
    "                \n",
    "                #set back to zero for next epoch (accumulate all over again)\n",
    "                self.magnitudes[frequency] *= 0.0\n",
    "            print(\"Re-aligned \",sum(re_aligned_frequencies_total)/len(re_aligned_frequencies_total),\" frequencies on average\")\n",
    "        #now do the step itself\n",
    "        return self.generic_step(loaded_data, batchidx, test=False)\n",
    "    \n",
    "    def on_train_epoch_start(self):\n",
    "        self.print(\"\\n\\n*** STARTING EPOCH \"+str(self.current_epoch)+\" ***\")\n",
    "        self.index_count = self.index_count * 0\n",
    "        self.unique_indices = set()\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        #eval everything\n",
    "        if self.global_rank == 0:\n",
    "            self.print(\"*** DONE WITH EPOCH \",self.current_epoch,\" AFTER \",self.TOTAL_ITS,\" ITS ***\")\n",
    "            \n",
    "            its_goal = 0\n",
    "            if self.SETTINGS.DATASET == \"CELEB\":\n",
    "                its_goal = 70\n",
    "            elif \"IMAGENET\" in self.SETTINGS.DATASET:\n",
    "                its_goal = 10000\n",
    "            else:\n",
    "                its_goal = 100\n",
    "\n",
    "            time_run = time.time() - self.start_epoch\n",
    "            print(\"PROGNOSTICATED TIME LEFT FOR \",its_goal,\" ITS: \",(time_run / (self.current_epoch * its_goal + 0.0001)  - time_run)/60.0/60.0,\" HOURS\")\n",
    "            \n",
    "            self.print(\"\\tUNIQUE INDICES IN TRAINING: \",len(self.unique_indices))\n",
    "            if self.SETTINGS.USE_INDIVIDUAL_CODEBOOKS:\n",
    "                no_unique = (self.index_count > 0).sum(dim=1)\n",
    "                print(\"\\tNo. of elements in each frequency band, min/avg/max: \",no_unique.min().item(),\"/\",no_unique.float().mean().item(),\"/\",no_unique.max().item())\n",
    "            self.print(\"\\tTIME TAKEN: \",time.time() - self.epoch_start_time,\" SECONDS\")\n",
    "            self.print(\"\\tLosses TRAIN:\")\n",
    "            self.print(\"\\t\\tRec: \",self.trainer.callback_metrics[\"train_loss_rec\"].item())\n",
    "            self.print(\"\\t\\tComm: \",self.trainer.callback_metrics[\"train_loss_code\"].item())\n",
    "            self.print(\"\\t\\tCode: \",self.trainer.callback_metrics[\"train_loss_comm\"].item())\n",
    "            if self.APPLY_GAN:\n",
    "                self.print(\"\\t\\t\\tGAN Losses:\")\n",
    "                self.print(\"\\t\\t\\t\\tDisc: \",self.trainer.callback_metrics[\"train_loss_disc\"].item())\n",
    "                self.print(\"\\t\\t\\t\\tGen: \",self.trainer.callback_metrics[\"train_loss_gan\"].item())\n",
    "                \n",
    "            if not SETTINGS.USE_GAN:\n",
    "                self.print(\"\\tLosses TEST:\")\n",
    "                self.print(\"\\t\\tRec / PSNR: \",self.trainer.callback_metrics[\"test_loss_rec\"].item())\n",
    "                self.print(\"\\t\\tComm: \",self.trainer.callback_metrics[\"test_loss_code\"].item())\n",
    "                self.print(\"\\t\\tCode: \",self.trainer.callback_metrics[\"test_loss_comm\"].item())\n",
    "                if self.best_test == None or self.best_test < self.trainer.callback_metrics[\"test_loss_rec\"].item():\n",
    "                    print(\"--> FOUND NEW BEST TEST LOSS: \",self.trainer.callback_metrics[\"test_loss_rec\"].item())\n",
    "                    print(\"\\t...saving...\")\n",
    "                    self.best_test = self.trainer.callback_metrics[\"test_loss_rec\"].item()\n",
    "                    torch.save(self.ae.state_dict(), \"outputs/\"+self.SETTINGS.RUN_NAME+\"/best.net\")\n",
    "\n",
    "            if self.current_epoch == its_goal:\n",
    "                print(\"--> SHUTTING DOWN\")\n",
    "                torch.save(self.ae.state_dict(), \"outputs/\"+self.SETTINGS.RUN_NAME+\"/last.net\")\n",
    "                sys.exit(0)\n",
    "                asdf\n",
    "            if self.APPLY_GAN and self.current_epoch % 5 == 0:\n",
    "                print(\"--> SAVING DOWN\")\n",
    "                torch.save(self.ae.state_dict(), \"outputs/\"+self.SETTINGS.RUN_NAME+\"/\"+str(self.current_epoch)+\".net\")\n",
    "        \n",
    "    #for debugging purposes:\n",
    "    #def on_after_backward(self) -> None:\n",
    "    #    for name, p in self.ae.named_parameters():\n",
    "    #        if p.grad is None and p.requires_grad:\n",
    "    #            print(name)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        LR = 0.0002\n",
    "        if SETTINGS.USE_GAN:\n",
    "            LR = 0.0002\n",
    "        #IF IMAGENET: scale down\n",
    "        if SETTINGS.DATASET in [\"IMAGENET\", \"CELEB\", \"CELEB64\"]:\n",
    "            LR = 0.0002\n",
    "        \n",
    "        if SETTINGS.LR != None:\n",
    "            LR = SETTINGS.LR\n",
    "        optim_ae = torch.optim.AdamW(self.ae.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "        if SETTINGS.USE_GAN:\n",
    "            optim_dis = torch.optim.AdamW(self.discriminator.parameters(), lr=LR, weight_decay=0.01)\n",
    "            self.discriminator.train(False)\n",
    "\n",
    "            return optim_ae, optim_dis\n",
    "        \n",
    "        return optim_ae\n",
    "\n",
    "def get_GQAE_parallel(SETTINGS):\n",
    "    no_limited_batches_train = None\n",
    "    no_limited_batches_test  = None\n",
    "    if \"IMAGENET\" in SETTINGS.DATASET: #don't train forever\n",
    "        no_limited_batches_train = 0.1\n",
    "        no_limited_batches_test  = 0.1\n",
    "        if SETTINGS.USE_GAN:\n",
    "            no_limited_batches_test  = 0.0\n",
    "\n",
    "    \n",
    "    its_goal = 0\n",
    "    if SETTINGS.DATASET == \"CELEB64\":\n",
    "        its_goal = 70\n",
    "    elif \"IMAGENET\" in SETTINGS.DATASET or SETTINGS.DATASET == \"CELEB\":\n",
    "        its_goal = 1000000\n",
    "    else:\n",
    "        its_goal = 100\n",
    "    its_goal += 1\n",
    "    \n",
    "    ae = BaseAE(SETTINGS, c_in=SETTINGS.INPUT_C)\n",
    "    \n",
    "    if SETTINGS.USE_GAN:\n",
    "        discriminator = Discriminator(SETTINGS, SETTINGS.INPUT_C, 16, SETTINGS.INPUT_W)\n",
    "    else:\n",
    "        discriminator = None\n",
    "    \n",
    "    parallel_net = Parallel_GQAE(SETTINGS, ae, discriminator)\n",
    "\n",
    "    if SETTINGS.CLUSTERRUN:\n",
    "        trainer = L.Trainer(limit_train_batches=no_limited_batches_train, limit_val_batches=no_limited_batches_test, max_epochs=its_goal, devices=torch.cuda.device_count(), accelerator=\"gpu\", enable_progress_bar=False, enable_checkpointing=SETTINGS.USE_CHECKPOINTS, logger=False, strategy=\"ddp_find_unused_parameters_true\")\n",
    "    else:\n",
    "        trainer = L.Trainer(limit_train_batches=no_limited_batches_train, limit_val_batches=no_limited_batches_test, max_epochs=its_goal, devices=torch.cuda.device_count(), accelerator=\"gpu\", enable_progress_bar=False, enable_checkpointing=SETTINGS.USE_CHECKPOINTS, logger=False)\n",
    "\n",
    "    trainer.fit(model=parallel_net, train_dataloaders=trainloader, val_dataloaders=testloader)\n",
    "\n",
    "    print(\"--> DONE TRAINING! shutting down...\")\n",
    "    sys.exit(0)\n",
    "get_GQAE_parallel(SETTINGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
